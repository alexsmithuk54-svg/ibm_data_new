{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Removing Duplicates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will focus on data wrangling, an important step in preparing data for analysis. Data wrangling involves cleaning and organizing data to make it suitable for analysis. One key task in this process is removing duplicate entries, which are repeated entries that can distort analysis and lead to inaccurate conclusions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will perform the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify duplicate rows  in the dataset.\n",
    "2. Use suitable techniques to remove duplicate rows and verify the removal.\n",
    "3. Summarize how to handle missing values appropriately.\n",
    "4. Use ConvertedCompYearly to normalize compensation data.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset into a DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the dataset using pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseId                      MainBranch                 Age  \\\n",
      "0           1  I am a developer by profession  Under 18 years old   \n",
      "1           2  I am a developer by profession     35-44 years old   \n",
      "2           3  I am a developer by profession     45-54 years old   \n",
      "3           4           I am learning to code     18-24 years old   \n",
      "4           5  I am a developer by profession     18-24 years old   \n",
      "\n",
      "            Employment RemoteWork   Check  \\\n",
      "0  Employed, full-time     Remote  Apples   \n",
      "1  Employed, full-time     Remote  Apples   \n",
      "2  Employed, full-time     Remote  Apples   \n",
      "3   Student, full-time        NaN  Apples   \n",
      "4   Student, full-time        NaN  Apples   \n",
      "\n",
      "                                    CodingActivities  \\\n",
      "0                                              Hobby   \n",
      "1  Hobby;Contribute to open-source projects;Other...   \n",
      "2  Hobby;Contribute to open-source projects;Other...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0                          Primary/elementary school   \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "3  Some college/university study without earning ...   \n",
      "4  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                           LearnCode  \\\n",
      "0                             Books / Physical media   \n",
      "1  Books / Physical media;Colleague;On the job tr...   \n",
      "2  Books / Physical media;Colleague;On the job tr...   \n",
      "3  Other online resources (e.g., videos, blogs, f...   \n",
      "4  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                     LearnCodeOnline  ... JobSatPoints_6  \\\n",
      "0                                                NaN  ...            NaN   \n",
      "1  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
      "2  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
      "3  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
      "4  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "\n",
      "  JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
      "0            NaN            NaN            NaN             NaN   \n",
      "1            0.0            0.0            0.0             0.0   \n",
      "2            NaN            NaN            NaN             NaN   \n",
      "3            NaN            NaN            NaN             NaN   \n",
      "4            NaN            NaN            NaN             NaN   \n",
      "\n",
      "  JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly JobSat  \n",
      "0             NaN                    NaN        NaN                 NaN    NaN  \n",
      "1             0.0                    NaN        NaN                 NaN    NaN  \n",
      "2             NaN  Appropriate in length       Easy                 NaN    NaN  \n",
      "3             NaN               Too long       Easy                 NaN    NaN  \n",
      "4             NaN              Too short       Easy                 NaN    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the dataset\n",
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to ensure it loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: If you are working on a local Jupyter environment, you can use the URL directly in the <code>pandas.read_csv()</code>  function as shown below:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Identifying Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Identify Duplicate Rows**\n",
    "  1. Count the number of duplicate rows in the dataset.\n",
    "  2. Display the first few duplicate rows to understand their structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Total duplicate rows (complete duplicates): 0\n",
      "\n",
      "2. No complete duplicate rows found.\n",
      "\n",
      "3. Checking duplicates in key columns:\n",
      "   - ResponseId duplicates: 0\n",
      "   - ResponseId: 0 duplicates, 65437 unique values\n",
      "   - MainBranch: 65432 duplicates, 5 unique values\n",
      "   - Employment: 65327 duplicates, 110 unique values\n",
      "   - RemoteWork: 65433 duplicates, 3 unique values\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "total_duplicates = df.duplicated().sum()\n",
    "print(f\"1. Total duplicate rows (complete duplicates): {total_duplicates}\")\n",
    "\n",
    "\n",
    "if total_duplicates > 0:\n",
    "    print(\"\\n2. First few duplicate rows:\")\n",
    "    duplicate_rows = df[df.duplicated(keep=False)]\n",
    "    print(duplicate_rows.head())\n",
    "else:\n",
    "    print(\"\\n2. No complete duplicate rows found.\")\n",
    "\n",
    "print(\"\\n3. Checking duplicates in key columns:\")\n",
    "\n",
    "\n",
    "if 'ResponseId' in df.columns:\n",
    "    responseid_duplicates = df['ResponseId'].duplicated().sum()\n",
    "    print(f\"   - ResponseId duplicates: {responseid_duplicates}\")\n",
    "    \n",
    "columns_to_check = ['ResponseId', 'MainBranch', 'Employment', 'RemoteWork']\n",
    "for col in columns_to_check:\n",
    "    if col in df.columns:\n",
    "        duplicates = df[col].duplicated().sum()\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"   - {col}: {duplicates} duplicates, {unique_count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Removing Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Remove Duplicates**\n",
    "   1. Remove duplicate rows from the dataset using the drop_duplicates() function.\n",
    "2. Verify the removal by counting the number of duplicate rows after removal .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows: 65437\n",
      "\n",
      "1. After removing complete duplicates:\n",
      "   Removed 0 complete duplicate rows\n",
      "   Remaining rows: 65437\n",
      "\n",
      "2. No ResponseId duplicates to remove.\n",
      "\n",
      "3. Verification after cleaning:\n",
      "   Complete duplicate rows remaining: 0\n",
      "   ResponseId duplicates remaining: 0\n",
      "\n",
      "4. Summary:\n",
      "   Initial rows: 65437\n",
      "   Final rows: 65437\n",
      "   Total rows removed: 0\n",
      "   Percentage removed: 0.00%\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "\n",
    "initial_rows = len(df)\n",
    "print(f\"Initial number of rows: {initial_rows}\")\n",
    "\n",
    "df_clean = df.drop_duplicates()\n",
    "rows_after_full_duplicate_removal = len(df_clean)\n",
    "full_duplicates_removed = initial_rows - rows_after_full_duplicate_removal\n",
    "\n",
    "print(f\"\\n1. After removing complete duplicates:\")\n",
    "print(f\"   Removed {full_duplicates_removed} complete duplicate rows\")\n",
    "print(f\"   Remaining rows: {rows_after_full_duplicate_removal}\")\n",
    "\n",
    "if 'ResponseId' in df_clean.columns:\n",
    "    responseid_duplicates_before = df_clean['ResponseId'].duplicated().sum()\n",
    "    \n",
    "    if responseid_duplicates_before > 0:\n",
    "        df_clean = df_clean.drop_duplicates(subset=['ResponseId'], keep='first')\n",
    "        rows_after_responseid_clean = len(df_clean)\n",
    "        responseid_duplicates_removed = rows_after_full_duplicate_removal - rows_after_responseid_clean\n",
    "        \n",
    "        print(f\"\\n2. After removing ResponseId duplicates:\")\n",
    "        print(f\"   Removed {responseid_duplicates_removed} rows with duplicate ResponseId\")\n",
    "        print(f\"   Remaining rows: {rows_after_responseid_clean}\")\n",
    "    else:\n",
    "        print(f\"\\n2. No ResponseId duplicates to remove.\")\n",
    "        rows_after_responseid_clean = rows_after_full_duplicate_removal\n",
    "\n",
    "print(f\"\\n3. Verification after cleaning:\")\n",
    "print(f\"   Complete duplicate rows remaining: {df_clean.duplicated().sum()}\")\n",
    "\n",
    "if 'ResponseId' in df_clean.columns:\n",
    "    print(f\"   ResponseId duplicates remaining: {df_clean['ResponseId'].duplicated().sum()}\")\n",
    "\n",
    "total_rows_removed = initial_rows - len(df_clean)\n",
    "print(f\"\\n4. Summary:\")\n",
    "print(f\"   Initial rows: {initial_rows}\")\n",
    "print(f\"   Final rows: {len(df_clean)}\")\n",
    "print(f\"   Total rows removed: {total_rows_removed}\")\n",
    "print(f\"   Percentage removed: {(total_rows_removed/initial_rows*100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Identify and Handle Missing Values**\n",
    "   1. Identify missing values for all columns in the dataset.\n",
    "   2. Choose a column with significant missing values (e.g., EdLevel) and impute with the most frequent value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 5: HANDLING MISSING VALUES ===\n",
      "1. Missing values in each column:\n",
      "                               Missing Values  Percentage\n",
      "AINextMuch less integrated              64289   98.245641\n",
      "AINextLess integrated                   63082   96.401119\n",
      "AINextNo change                         52939   80.900714\n",
      "AINextMuch more integrated              51999   79.464217\n",
      "EmbeddedAdmired                         48704   74.428840\n",
      "EmbeddedWantToWorkWith                  47837   73.103901\n",
      "EmbeddedHaveWorkedWith                  43223   66.052845\n",
      "ConvertedCompYearly                     42002   64.186928\n",
      "AIToolNot interested in Using           41023   62.690832\n",
      "AINextMore integrated                   41009   62.669438\n",
      "Knowledge_9                             37802   57.768541\n",
      "Frequency_3                             37727   57.653927\n",
      "Knowledge_8                             37679   57.580574\n",
      "ProfessionalTech                        37673   57.571405\n",
      "Knowledge_7                             37659   57.550010\n",
      "\n",
      "2. Handling missing values in 'EdLevel' column:\n",
      "   Missing values before imputation: 4653 (7.11%)\n",
      "   Most frequent education level: 'Bachelor’s degree (B.A., B.S., B.Eng., etc.)'\n",
      "   Missing values after imputation: 0\n",
      "\n",
      "   Education level distribution after imputation:\n",
      "   - Bachelor’s degree (B.A., B.S., B.Eng., etc.): 29595 (45.2%)\n",
      "   - Master’s degree (M.A., M.S., M.Eng., MBA, etc.): 15557 (23.8%)\n",
      "   - Some college/university study without earning a degree: 7651 (11.7%)\n",
      "   - Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.): 5793 (8.9%)\n",
      "   - Professional degree (JD, MD, Ph.D, Ed.D, etc.): 2970 (4.5%)\n",
      "   - Associate degree (A.A., A.S., etc.): 1793 (2.7%)\n",
      "   - Primary/elementary school: 1146 (1.8%)\n",
      "   - Something else: 932 (1.4%)\n",
      "\n",
      "3. Handling missing values in 'ConvertedCompYearly' column:\n",
      "   Missing values before: 42002 (64.19%)\n",
      "   Will handle in Step 6: Normalizing Compensation Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_719/3150143199.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean['EdLevel'].fillna(most_frequent_edlevel, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "# your code goes here\n",
    "print(\"\\n=== STEP 5: HANDLING MISSING VALUES ===\")\n",
    "\n",
    "# 1. Identify missing values for all columns\n",
    "print(\"1. Missing values in each column:\")\n",
    "missing_values = df_clean.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df_clean)) * 100\n",
    "\n",
    "# Create a dataframe for missing values summary\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "# Show columns with missing values (sorted by percentage)\n",
    "missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values('Percentage', ascending=False)\n",
    "print(missing_df.head(15))  # Show top 15 columns with most missing values\n",
    "\n",
    "# 2. Choose a column with significant missing values - EdLevel\n",
    "print(f\"\\n2. Handling missing values in 'EdLevel' column:\")\n",
    "\n",
    "if 'EdLevel' in df_clean.columns:\n",
    "    # Check current missing values\n",
    "    edlevel_missing_before = df_clean['EdLevel'].isnull().sum()\n",
    "    edlevel_missing_percentage = (edlevel_missing_before / len(df_clean)) * 100\n",
    "    \n",
    "    print(f\"   Missing values before imputation: {edlevel_missing_before} ({edlevel_missing_percentage:.2f}%)\")\n",
    "    \n",
    "    # Find the most frequent value (mode)\n",
    "    most_frequent_edlevel = df_clean['EdLevel'].mode()[0]\n",
    "    print(f\"   Most frequent education level: '{most_frequent_edlevel}'\")\n",
    "    \n",
    "    # Impute missing values with the most frequent value\n",
    "    df_clean['EdLevel'].fillna(most_frequent_edlevel, inplace=True)\n",
    "    \n",
    "    # Check after imputation\n",
    "    edlevel_missing_after = df_clean['EdLevel'].isnull().sum()\n",
    "    print(f\"   Missing values after imputation: {edlevel_missing_after}\")\n",
    "    \n",
    "    # Show distribution of EdLevel after imputation\n",
    "    print(f\"\\n   Education level distribution after imputation:\")\n",
    "    edlevel_counts = df_clean['EdLevel'].value_counts()\n",
    "    for level, count in edlevel_counts.head(10).items():\n",
    "        percentage = (count / len(df_clean)) * 100\n",
    "        print(f\"   - {level}: {count} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"   'EdLevel' column not found in dataset.\")\n",
    "    \n",
    "# 3. Optional: Handle missing values in another important column\n",
    "print(f\"\\n3. Handling missing values in 'ConvertedCompYearly' column:\")\n",
    "\n",
    "if 'ConvertedCompYearly' in df_clean.columns:\n",
    "    # Check current missing values\n",
    "    comp_missing_before = df_clean['ConvertedCompYearly'].isnull().sum()\n",
    "    comp_missing_percentage = (comp_missing_before / len(df_clean)) * 100\n",
    "    \n",
    "    print(f\"   Missing values before: {comp_missing_before} ({comp_missing_percentage:.2f}%)\")\n",
    "    \n",
    "    # We'll handle this in the next step\n",
    "    print(\"   Will handle in Step 6: Normalizing Compensation Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Normalizing Compensation Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Normalize Compensation Data Using ConvertedCompYearly**\n",
    "   1. Use the ConvertedCompYearly column for compensation analysis as the normalized annual compensation is already provided.\n",
    "   2. Check for missing values in ConvertedCompYearly and handle them if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 6: NORMALIZING COMPENSATION DATA ===\n",
      "1. ConvertedCompYearly column analysis:\n",
      "   Data type: float64\n",
      "   Missing values: 42002 (64.19%)\n",
      "\n",
      "   Basic statistics (excluding missing values):\n",
      "   Count:    23435\n",
      "   Mean:     $86,155.29\n",
      "   Std:      $186,756.97\n",
      "   Min:      $1.00\n",
      "   25%:      $32,712.00\n",
      "   50%:      $65,000.00\n",
      "   75%:      $107,971.50\n",
      "   Max:      $16,256,603.00\n",
      "\n",
      "2. Handling missing values in ConvertedCompYearly:\n",
      "   Option 2: Imputed 42002 missing values with median: $65,000.00\n",
      "\n",
      "3. Compensation distribution analysis:\n",
      "   Compensation categories:\n",
      "   - <50k: 8798 respondents (13.4%)\n",
      "   - 50k-100k: 49987 respondents (76.4%)\n",
      "   - 100k-150k: 3651 respondents (5.6%)\n",
      "   - 150k-200k: 1714 respondents (2.6%)\n",
      "   - 200k-300k: 896 respondents (1.4%)\n",
      "   - 300k+: 391 respondents (0.6%)\n",
      "\n",
      "4. Average compensation by employment status:\n",
      "   - Employed, full-time;Independent contractor, freelancer, or self-employed;Student, part-time;Retired: $803,285.00\n",
      "   - Employed, full-time;Independent contractor, freelancer, or self-employed;Employed, part-time: $134,579.42\n",
      "   - Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Retired: $127,929.56\n",
      "   - Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work: $107,851.20\n",
      "   - Employed, full-time;Not employed, and not looking for work: $105,000.00\n",
      "   - Independent contractor, freelancer, or self-employed;Employed, part-time: $103,069.56\n",
      "   - Independent contractor, freelancer, or self-employed;Employed, part-time;Retired: $88,000.00\n",
      "   - Employed, full-time;Independent contractor, freelancer, or self-employed: $76,227.45\n",
      "   - Employed, full-time: $75,775.25\n",
      "   - Independent contractor, freelancer, or self-employed: $75,498.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_719/919317382.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_final['ConvertedCompYearly'].fillna(median_comp, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Cleaned dataset saved as 'survey_data_cleaned_final.csv'\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "# your code goes here\n",
    "print(\"\\n=== STEP 6: NORMALIZING COMPENSATION DATA ===\")\n",
    "\n",
    "# 1. Check if ConvertedCompYearly column exists\n",
    "if 'ConvertedCompYearly' in df_clean.columns:\n",
    "    print(\"1. ConvertedCompYearly column analysis:\")\n",
    "    \n",
    "    # Check data type and basic statistics\n",
    "    print(f\"   Data type: {df_clean['ConvertedCompYearly'].dtype}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    comp_missing = df_clean['ConvertedCompYearly'].isnull().sum()\n",
    "    comp_missing_percentage = (comp_missing / len(df_clean)) * 100\n",
    "    print(f\"   Missing values: {comp_missing} ({comp_missing_percentage:.2f}%)\")\n",
    "    \n",
    "    # Show basic statistics (excluding NaN)\n",
    "    print(f\"\\n   Basic statistics (excluding missing values):\")\n",
    "    comp_stats = df_clean['ConvertedCompYearly'].describe()\n",
    "    print(f\"   Count:    {comp_stats['count']:.0f}\")\n",
    "    print(f\"   Mean:     ${comp_stats['mean']:,.2f}\")\n",
    "    print(f\"   Std:      ${comp_stats['std']:,.2f}\")\n",
    "    print(f\"   Min:      ${comp_stats['min']:,.2f}\")\n",
    "    print(f\"   25%:      ${comp_stats['25%']:,.2f}\")\n",
    "    print(f\"   50%:      ${comp_stats['50%']:,.2f}\")\n",
    "    print(f\"   75%:      ${comp_stats['75%']:,.2f}\")\n",
    "    print(f\"   Max:      ${comp_stats['max']:,.2f}\")\n",
    "    \n",
    "    # 2. Handle missing values - different strategies\n",
    "    print(f\"\\n2. Handling missing values in ConvertedCompYearly:\")\n",
    "    \n",
    "    # Option 1: Remove rows with missing compensation (if not too many)\n",
    "    if comp_missing_percentage < 30:  # If less than 30% missing\n",
    "        df_comp_clean = df_clean.dropna(subset=['ConvertedCompYearly'])\n",
    "        rows_removed = len(df_clean) - len(df_comp_clean)\n",
    "        print(f\"   Option 1: Removed {rows_removed} rows with missing compensation\")\n",
    "        print(f\"   Rows remaining: {len(df_comp_clean)}\")\n",
    "        \n",
    "        # Use this cleaned dataset for compensation analysis\n",
    "        df_final = df_comp_clean.copy()\n",
    "    else:\n",
    "        # Option 2: Impute with median (if many missing)\n",
    "        median_comp = df_clean['ConvertedCompYearly'].median()\n",
    "        df_final = df_clean.copy()\n",
    "        df_final['ConvertedCompYearly'].fillna(median_comp, inplace=True)\n",
    "        print(f\"   Option 2: Imputed {comp_missing} missing values with median: ${median_comp:,.2f}\")\n",
    "    \n",
    "    # 3. Analyze compensation distribution\n",
    "    print(f\"\\n3. Compensation distribution analysis:\")\n",
    "    \n",
    "    # Create compensation categories\n",
    "    df_final['CompCategory'] = pd.cut(df_final['ConvertedCompYearly'], \n",
    "                                       bins=[0, 50000, 100000, 150000, 200000, 300000, float('inf')],\n",
    "                                       labels=['<50k', '50k-100k', '100k-150k', '150k-200k', '200k-300k', '300k+'])\n",
    "    \n",
    "    # Show distribution\n",
    "    comp_distribution = df_final['CompCategory'].value_counts().sort_index()\n",
    "    print(\"   Compensation categories:\")\n",
    "    for category, count in comp_distribution.items():\n",
    "        percentage = (count / len(df_final)) * 100\n",
    "        print(f\"   - {category}: {count} respondents ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 4. Compensation by Employment status (if column exists)\n",
    "    if 'Employment' in df_final.columns:\n",
    "        print(f\"\\n4. Average compensation by employment status:\")\n",
    "        \n",
    "        # Calculate mean compensation for each employment type\n",
    "        comp_by_employment = df_final.groupby('Employment')['ConvertedCompYearly'].mean().sort_values(ascending=False)\n",
    "        \n",
    "        for employment, avg_comp in comp_by_employment.head(10).items():\n",
    "            print(f\"   - {employment}: ${avg_comp:,.2f}\")\n",
    "    \n",
    "    # 5. Save cleaned dataset\n",
    "    df_final.to_csv('survey_data_cleaned_final.csv', index=False)\n",
    "    print(f\"\\n5. Cleaned dataset saved as 'survey_data_cleaned_final.csv'\")\n",
    "    \n",
    "else:\n",
    "    print(\"ConvertedCompYearly column not found in dataset.\")\n",
    "    df_final = df_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, you focused on identifying and removing duplicate rows.**\n",
    "\n",
    "- You handled missing values by imputing the most frequent value in a chosen column.\n",
    "\n",
    "- You used ConvertedCompYearly for compensation normalization and handled missing values.\n",
    "\n",
    "- For further analysis, consider exploring other columns or visualizing the cleaned dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATASET STATISTICS AFTER CLEANING:\n",
      "-----------------------------------\n",
      "• Final rows: 65,437\n",
      "• Final columns: 115\n",
      "• Average compensation: $72,576.36\n",
      "• Median compensation: $65,000.00\n",
      "• Unique education levels: 8\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "print(\"\\nDATASET STATISTICS AFTER CLEANING:\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"• Final rows: {len(df_final):,}\")\n",
    "print(f\"• Final columns: {len(df_final.columns):,}\")\n",
    "\n",
    "if 'ConvertedCompYearly' in df_final.columns:\n",
    "    print(f\"• Average compensation: ${df_final['ConvertedCompYearly'].mean():,.2f}\")\n",
    "    print(f\"• Median compensation: ${df_final['ConvertedCompYearly'].median():,.2f}\")\n",
    "\n",
    "if 'EdLevel' in df_final.columns:\n",
    "    unique_edlevels = df_final['EdLevel'].nunique()\n",
    "    print(f\"• Unique education levels: {unique_edlevels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-11-05|1.2|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-24|1.1|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-23|1.0|Raghul Ramesh|Created lab|\n",
    "\n",
    "--!>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "2116052544ce403759eef2159eb3d21f1d38e895d652bcaffa36a5791482361d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
